{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "regulation-relationship",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "warming-parent",
   "metadata": {},
   "source": [
    "## Necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "early-hours",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "wanted-branch",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "descending-danger",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/big/Desktop/Aulas\n",
      "total 331320\n",
      "drwxrwxr-x 5 big big      4096 mai 23 03:11  .\n",
      "drwxr-xr-x 3 big big      4096 mai 18 22:55  ..\n",
      "drwxrwxr-x 3 big big      4096 mai 21 14:16  aula_08-20210517T134909Z-001\n",
      "-rwxrw-rw- 1 big big     15277 mai 23 03:11 'Data Preparation.ipynb'\n",
      "drwxrwxr-x 2 big big      4096 mai 23 02:57  .ipynb_checkpoints\n",
      "drwxrwxr-x 7 big big      4096 mai 23 03:04  Projeto\n",
      "-rw-rw-r-- 1 big big 339230587 mai 18 20:18  Projeto.zip\n",
      "head: cannot open 'amazon_item_ratings.csv' for reading: No such file or directory\n",
      "tail: cannot open 'amazon_item_ratings.csv' for reading: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "! pwd\n",
    "! ls -la\n",
    "! head -n 3 amazon_item_ratings.csv\n",
    "! tail -n 3 amazon_item_ratings.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessible-custody",
   "metadata": {},
   "source": [
    "## Read dataset from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "moderate-appliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_items = spark.read.csv(\"amazon_item_ratings.csv\", header=False, inferSchema=True, sep=\",\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bigger-selection",
   "metadata": {},
   "source": [
    "Get a fraction of the sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dynamic-prophet",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_items = df_items.sample(fraction=0.05) #0.05 works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convenient-mention",
   "metadata": {},
   "source": [
    "## Multiple checks on structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unsigned-pointer",
   "metadata": {},
   "source": [
    "Check dataset schema and column datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "collaborative-ratio",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: double (nullable = true)\n",
      " |-- _c3: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "826913"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_items.printSchema()\n",
    "df_items.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coral-qualification",
   "metadata": {},
   "source": [
    "Display 10 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "synthetic-result",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_items.show(n=10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ahead-seven",
   "metadata": {},
   "source": [
    "Change column names to improve readabilty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "declared-powder",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_items = df_items.withColumnRenamed(\"_c0\",\"Reviewer\") \\\n",
    "    .withColumnRenamed(\"_c1\",\"Item\") \\\n",
    "    .withColumnRenamed(\"_c2\",\"Rating\") \\\n",
    "    .withColumnRenamed(\"_c3\",\"Timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "necessary-identifier",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_items.show(n=10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sought-style",
   "metadata": {},
   "source": [
    "Check for Null or NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "rotary-weapon",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+\n",
      "|_c0|_c1|_c2|_c3|\n",
      "+---+---+---+---+\n",
      "|  0|  0|  0|  0|\n",
      "+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_items.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_items.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "registered-citizen",
   "metadata": {},
   "source": [
    "Since there are no Null or NaN values we can safely convert \"Rating\" column to an Integer since Item ratings can only be natural numbers ranging from 1 to 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minus-somalia",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_items = df_items.withColumn(\"Rating\", df_items[\"Rating\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "better-programmer",
   "metadata": {},
   "source": [
    "Check if .cast() was successful and look for values out of place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "latter-chassis",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|               _c2|\n",
      "+-------+------------------+\n",
      "|  count|            826913|\n",
      "|   mean| 4.162625330597052|\n",
      "| stddev|1.2614705476479164|\n",
      "|    min|               1.0|\n",
      "|    max|               5.0|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_items.describe(\"Rating\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extraordinary-crime",
   "metadata": {},
   "source": [
    "## Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colonial-pension",
   "metadata": {},
   "source": [
    "Two methods were tried since we were never able to run StringIndexer on more than 10% of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaging-contributor",
   "metadata": {},
   "source": [
    "### PySpark Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mediterranean-encoding",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexers = [StringIndexer(inputCol=\"Reviewer\", outputCol=\"ReviewerID\", handleInvalid=\"skip\") , StringIndexer(inputCol=\"Item\", outputCol=\"ItemID\", handleInvalid=\"skip\")]\n",
    "\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "df_items_indexed = pipeline.fit(df_items).transform(df_items)\n",
    "\n",
    "df_items_indexed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stylish-amazon",
   "metadata": {},
   "source": [
    "### One column at a time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overall-coach",
   "metadata": {},
   "source": [
    "For some reason that allows StringIndexer to fit a greater sample than Pipeline before eventually Java runs out of memory or KryoSerializer throws a BufferOverflowing error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "juvenile-bunny",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|_c1       |Item_Index|\n",
      "+----------+----------+\n",
      "|0000000078|109757.0  |\n",
      "|B007PQTYIG|442985.0  |\n",
      "|0735623872|145094.0  |\n",
      "|030740515X|7460.0    |\n",
      "|B003HAL5ZO|360858.0  |\n",
      "|0399536957|129965.0  |\n",
      "|B004HO58UW|41982.0   |\n",
      "|B003JQLG4Q|362137.0  |\n",
      "|B0041MUB52|373762.0  |\n",
      "|B009XZ9Q1C|46018.0   |\n",
      "|B007ZJ1M9C|447558.0  |\n",
      "|B003A845OQ|356169.0  |\n",
      "|0972973052|167851.0  |\n",
      "|B002P3YRAY|84633.0   |\n",
      "|0888550081|162628.0  |\n",
      "|0521697522|136584.0  |\n",
      "|B0049LUI9O|523.0     |\n",
      "|B00ATSSQT0|483915.0  |\n",
      "|B000HDK0DC|8264.0    |\n",
      "|B00DNUF7KW|5786.0    |\n",
      "+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indexerItem = StringIndexer(inputCol=\"Item\", outputCol=\"ItemID\")\n",
    "df_items_index_users = indexerItem.fit(df_items).transform(df_items)\n",
    "df_items_index_users.select(\"_c1\",\"Item_Index\").show(truncate=False)\n",
    "\n",
    "del df_items\n",
    "\n",
    "indexerUsers = StringIndexer(inputCol=\"Reviewer\", outputCol=\"ReviewerID\")\n",
    "df_items_indexed = indexerUsers.fit(df_items_index_users).transform(df_items_index_users)\n",
    "df_items_indexed.select(\"_c0\",\"User_Index\").show(truncate=False)\n",
    "\n",
    "del df_items_index_users"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggressive-blank",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "static-scholar",
   "metadata": {},
   "source": [
    "Double check schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "middle-miami",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: double (nullable = true)\n",
      " |-- _c3: integer (nullable = true)\n",
      " |-- Item_Index: double (nullable = false)\n",
      " |-- User_Index: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_items_indexed.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chronic-leave",
   "metadata": {},
   "source": [
    "Drop \"TimeStamp\" column since we are not going to use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "environmental-financing",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_items_indexed = df_items_indexed.drop(\"TimeStamp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moderate-filter",
   "metadata": {},
   "source": [
    "## Store data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greenhouse-mapping",
   "metadata": {},
   "source": [
    "Display 10 rules to make sure the DataFrame is as we want it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assured-fiction",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_items_indexed.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alleged-friendship",
   "metadata": {},
   "source": [
    "Save Dataframe do parquet format to use with SparkSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "specified-checkout",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_items = \"data.parquet\"\n",
    "df_items_indexed.write.mode(\"overwrite\").parquet(output_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "driven-cowboy",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Can not create the managed table('`ItemsTable`'). The associated location('file:/home/big/Desktop/Aulas/Projeto/spark-warehouse/itemstable') already exists.;",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-776f1fb4c6a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_items_indexed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"overwrite\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ItemsTable\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msaveAsTable\u001b[0;34m(self, name, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    866\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Can not create the managed table('`ItemsTable`'). The associated location('file:/home/big/Desktop/Aulas/Projeto/spark-warehouse/itemstable') already exists.;"
     ]
    }
   ],
   "source": [
    "df_items_indexed.write.mode(\"overwrite\").saveAsTable(\"DataTable\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
